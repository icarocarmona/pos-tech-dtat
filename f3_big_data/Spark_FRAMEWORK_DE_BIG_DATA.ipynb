{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBp7kt7Ing3nPiQeBj5rZT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/icarocarmona/pos-tech-dtat/blob/main/f3_big_data/Spark_FRAMEWORK_DE_BIG_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqdoID92qrDp",
        "outputId": "c14bf179-4603-40e8-bc9f-d8f7cf26ba87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=e33945306eff8341d77ddb56c1c93f2954ab7b3e15676ce73870944f45b459fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando a sessão do SparkContext e SparkSession"
      ],
      "metadata": {
        "id": "m5O6qRg6q6ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "evk_qR44qxBo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "Zk1en4UNrAvK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('DataFrame e RDD').getOrCreate()"
      ],
      "metadata": {
        "id": "p3gf9mddrHc2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create pyspark dataframe from an existing rdd\n",
        "\n",
        "rdd = sc.parallelize([(\"C\",85,76,87,91),(\"A\",35,46,67,91),(\"B\",87,86,37,94),(\"A\",15,63,75,16)], 4)"
      ],
      "metadata": {
        "id": "HatBZMC-rPpr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(rdd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXhFGRVNrUNF",
        "outputId": "7d34c579-ac9a-4e2b-a60d-569bbf74c2ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub = ['id_person', 'value_1', 'value_2', 'value_3', 'value_4']"
      ],
      "metadata": {
        "id": "l8dpTZyssF4S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "marks_df = spark.createDataFrame(rdd, schema=sub)"
      ],
      "metadata": {
        "id": "Endi9EcHsX8Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(marks_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-r8G-4HtDPd",
        "outputId": "f7c3e09b-6b97-4a81-d9c8-b328f49abfc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "marks_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUvvQzbRtK1p",
        "outputId": "a7302804-0a90-4dff-e557-a91d34477913"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id_person: string (nullable = true)\n",
            " |-- value_1: long (nullable = true)\n",
            " |-- value_2: long (nullable = true)\n",
            " |-- value_3: long (nullable = true)\n",
            " |-- value_4: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "marks_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-CZXNhhseIH",
        "outputId": "e320ba58-4f6a-44c1-d877-41c06cfa1501"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-------+-------+-------+\n",
            "|id_person|value_1|value_2|value_3|value_4|\n",
            "+---------+-------+-------+-------+-------+\n",
            "|        C|     85|     76|     87|     91|\n",
            "|        A|     35|     46|     67|     91|\n",
            "|        B|     87|     86|     37|     94|\n",
            "|        A|     15|     63|     75|     16|\n",
            "+---------+-------+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('cereal.csv', sep=',', inferSchema=True, header = True)"
      ],
      "metadata": {
        "id": "8YXdQW7ItM4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "90ce8c92-dd83-4f42-aac2-7a42ce637e0f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/cereal.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b933154ceb39>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cereal.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/cereal.csv."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "z-2K2Hrqvfx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select()"
      ],
      "metadata": {
        "id": "3YeT2UXrwLQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('Cereal Name','Manufacturer', 'Type' ).show()"
      ],
      "metadata": {
        "id": "BfVuGt2hwIrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# withColumn()"
      ],
      "metadata": {
        "id": "EvAdZFnswcXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('Name', df['Cereal Name'].cast(\"String\")).printSchema()"
      ],
      "metadata": {
        "id": "Dc692cn6wZkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# groupBy"
      ],
      "metadata": {
        "id": "GSF4qQLGxIB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('Cereal Name', 'Calories').count().show()"
      ],
      "metadata": {
        "id": "Bsy_lCbFxJo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# orderBy()"
      ],
      "metadata": {
        "id": "9zasH6tSxc3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.orderBy('Protein (g)').show()"
      ],
      "metadata": {
        "id": "WWtIMRCQxSzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case When"
      ],
      "metadata": {
        "id": "Ofm7cB3OxzhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when"
      ],
      "metadata": {
        "id": "dnhWlXS-xqYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"Cereal Name\", when(df['Vitamins and Minerals'] >= \"25\", \"rich in vitamins\").alias('rich in vitamins')).show()"
      ],
      "metadata": {
        "id": "W_3ehrlXx-l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# filter()"
      ],
      "metadata": {
        "id": "fO_JfIToyxvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df['Calories'] == '100').show()"
      ],
      "metadata": {
        "id": "AJ3vjO5cyd1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# isnull isnotnull"
      ],
      "metadata": {
        "id": "l0mfd1_Ozbdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "gVYLga_Szg6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df['Cereal Name'].isNotNull()).show()"
      ],
      "metadata": {
        "id": "wB2EsTtYzoQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df['Cereal Name'].isNull()).show()"
      ],
      "metadata": {
        "id": "ANttWZuVzyfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df['Cereal Name'].isNull()).count()"
      ],
      "metadata": {
        "id": "69VH_qp1z1c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark SQL Consultas e Seleções"
      ],
      "metadata": {
        "id": "vAar4c0gayBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").appName('Spark SQL').getOrCreate()"
      ],
      "metadata": {
        "id": "F7QUdEXUa8iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.sql(\"select 'OK' as Status\")"
      ],
      "metadata": {
        "id": "C0TLYgi0036-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "A6JYbqxjbIb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('cereal.csv', sep=',', inferSchema=True, header = True)"
      ],
      "metadata": {
        "id": "HRyAay_BbQTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "id": "Z1NDFvJNbaE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"cereal\")"
      ],
      "metadata": {
        "id": "wfaBDxMlbqWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"SELECT * FROM cereal where Type = 'C'\")\n",
        "cereal.show(5)"
      ],
      "metadata": {
        "id": "au0tCVMib0wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.where(df['Type'] == 'C').show(2)"
      ],
      "metadata": {
        "id": "BgOI7V7Hb_hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT Manufacturer FROM cereal group by Manufacturer\").show()"
      ],
      "metadata": {
        "id": "WLmNAjz4dZ9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT Count(*) as total FROM cereal where Manufacturer = 'General Mills'\").show()"
      ],
      "metadata": {
        "id": "QcE4492OdKGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"SELECT 'Cereal Name' as name, Type as type, Manufacturer as mfr  FROM cereal \").show()\n",
        "cereal"
      ],
      "metadata": {
        "id": "kwMQBpRsCmVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"SELECT 'Cereal Name' as name, Type as type , Manufacturer as mfr  FROM cereal \").show()\n",
        "cereal"
      ],
      "metadata": {
        "id": "cjQpNBUjEuPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"SELECT DISTINCT  Type as type , Manufacturer as mfr  FROM cereal \")\n",
        "cereal.count()\n"
      ],
      "metadata": {
        "id": "MnXpFAX1FE1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM cereal LIMIT 1\").show()"
      ],
      "metadata": {
        "id": "SBmDkhwVFpE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"SELECT  Manufacturer,  Type, COUNT(*) FROM cereal  GROUP BY Manufacturer,  Type\")\n",
        "cereal.show()\n"
      ],
      "metadata": {
        "id": "nG4cJoz3FgVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"SELECT  Manufacturer,  Type, COUNT(*) as total , SUM(Calories) FROM cereal  GROUP BY Manufacturer,  Type\")\n",
        "cereal.show()\n"
      ],
      "metadata": {
        "id": "a0i9-QRzGOiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"\"\"\n",
        "\n",
        "SELECT  DISTINCT Type from cereal\n",
        "\"\"\")\n",
        "cereal.show()\n"
      ],
      "metadata": {
        "id": "y8IVeCzUGrjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"\"\"\n",
        "SELECT  Manufacturer,\n",
        "  (\n",
        "    CASE\n",
        "      WHEN Type = 'C' THEN 'A'\n",
        "      WHEN Type = 'H' THEN 'B'\n",
        "      ELSE 'C'\n",
        "    END\n",
        "  ) AS Type_Name,\n",
        "  COUNT(*) AS Total,\n",
        "  SUM(Calories) AS Total_Calories\n",
        "FROM cereal\n",
        "GROUP BY\n",
        "  Manufacturer , Type\n",
        "\"\"\")\n",
        "cereal.show()"
      ],
      "metadata": {
        "id": "yl5nqjF6G1Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consultas Avançadas em Spark SQL\n"
      ],
      "metadata": {
        "id": "epfcvq17IsWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"\"\"\n",
        "SELECT  Manufacturer AS mfr,\n",
        "  Type AS type,\n",
        "  sum(Calories) AS sum_calories,\n",
        "  min(Calories) AS min_calories,\n",
        "  max(Calories) AS max_calories,\n",
        "  avg(Calories) AS avg_calories,\n",
        "  count(distinct `Cereal Name`) AS count_distinct_name,\n",
        "  count(`Cereal Name`) AS count_name\n",
        "FROM cereal\n",
        "GROUP BY Manufacturer, Type\n",
        "ORDER BY Manufacturer, Type\n",
        "\"\"\")\n",
        "cereal.show()"
      ],
      "metadata": {
        "id": "nHRtzKffIKOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"\"\"\n",
        "SELECT  Manufacturer AS mfr,\n",
        "  Type AS type,\n",
        "  sum(Calories) AS sum_calories,\n",
        "  min(Calories) AS min_calories,\n",
        "  max(Calories) AS max_calories,\n",
        "  cast(avg(Calories) as decimal(10,2)) AS avg_calories,\n",
        "  count(distinct `Cereal Name`) AS count_distinct_name,\n",
        "  count(`Cereal Name`) AS count_name\n",
        "FROM cereal\n",
        "GROUP BY Manufacturer, Type\n",
        "ORDER BY Manufacturer, Type\n",
        "\"\"\")\n",
        "cereal.show()"
      ],
      "metadata": {
        "id": "j2Mn1IADI_YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"\"\"\n",
        "SELECT  Manufacturer AS mfr,\n",
        "  Type AS type,\n",
        "\n",
        "  sum(Calories) AS sum_calories,\n",
        "  min(Calories) AS min_calories,\n",
        "  max(Calories) AS max_calories,\n",
        "  cast(avg(Calories) as decimal(10,2)) AS avg_calories,\n",
        "\n",
        "  sum(Carbs) AS sum_carbs,\n",
        "  min(Carbs) AS min_carbs,\n",
        "  max(Carbs) AS max_carbs,\n",
        "  cast(avg(Carbs) as decimal(10,2)) AS avg_carbs,\n",
        "\n",
        "  sum(`Vitamins and Minerals`) AS sum_vitamins,\n",
        "  min(`Vitamins and Minerals`) AS min_vitamins,\n",
        "  max(`Vitamins and Minerals`) AS max_vitamins,\n",
        "  cast(avg(`Vitamins and Minerals`) as decimal(10,2)) AS avg_vitamins,\n",
        "\n",
        "\n",
        "  count(distinct `Cereal Name`) AS count_distinct_name,\n",
        "  count(`Cereal Name`) AS count_name\n",
        "FROM cereal\n",
        "GROUP BY Manufacturer, Type\n",
        "ORDER BY Manufacturer, Type\n",
        "\"\"\")\n",
        "cereal.show()\n"
      ],
      "metadata": {
        "id": "hA880yjwLHtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cereal = spark.sql(\"\"\"\n",
        "SELECT  Manufacturer AS mfr,\n",
        "  Type AS type,\n",
        "  (\n",
        "    CASE\n",
        "      WHEN  substr(mfr,1,1) = 'A' then 'pera'\n",
        "      WHEN  mfr = 'A' then 'pera'\n",
        "      WHEN  mfr = 'A' then 'pera'\n",
        "      WHEN  mfr = 'A' then 'pera'\n",
        "      WHEN  mfr = 'A' then 'pera'\n",
        "      WHEN  mfr = 'A' then 'pera'\n",
        "      WHEN  mfr = 'A' then 'pera'\n",
        "      eLSE 'NA'\n",
        "    END\n",
        "  ) mfr_modified,\n",
        "  sum(Calories) AS sum_calories,\n",
        "  min(Calories) AS min_calories,\n",
        "  max(Calories) AS max_calories,\n",
        "  cast(avg(Calories) as decimal(10,2)) AS avg_calories,\n",
        "\n",
        "  sum(Carbs) AS sum_carbs,\n",
        "  min(Carbs) AS min_carbs,\n",
        "  max(Carbs) AS max_carbs,\n",
        "  cast(avg(Carbs) as decimal(10,2)) AS avg_carbs,\n",
        "\n",
        "  sum(`Vitamins and Minerals`) AS sum_vitamins,\n",
        "  min(`Vitamins and Minerals`) AS min_vitamins,\n",
        "  max(`Vitamins and Minerals`) AS max_vitamins,\n",
        "  cast(avg(`Vitamins and Minerals`) as decimal(10,2)) AS avg_vitamins,\n",
        "\n",
        "\n",
        "  count(distinct `Cereal Name`) AS count_distinct_name,\n",
        "  count(`Cereal Name`) AS count_name\n",
        "FROM cereal\n",
        "GROUP BY Manufacturer, Type\n",
        "ORDER BY Manufacturer, Type\n",
        "\"\"\")\n",
        "cereal.show()\n"
      ],
      "metadata": {
        "id": "f2VJ6NWXLpbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations"
      ],
      "metadata": {
        "id": "RUXxWzZ2qvuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1, 2, 3, 4,5]\n",
        "rdd = sc.parallelize(data)\n",
        "new_rdd = rdd.map(lambda x: x* 2)\n",
        "print(new_rdd.collect())"
      ],
      "metadata": {
        "id": "8sHKIPoRq3bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## filter()"
      ],
      "metadata": {
        "id": "La9q11BGrexP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1, 2, 3, 4,5, 6, 7, 8]\n",
        "rdd = sc.parallelize(data)\n",
        "new_rdd = rdd.filter(lambda x: x%2 == 0)\n",
        "print(new_rdd.collect())"
      ],
      "metadata": {
        "id": "kihEBLx4rTLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## distinct"
      ],
      "metadata": {
        "id": "WT4y7xznsOD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1, 2, 3, 4,5, 6, 7, 8]\n",
        "rdd = sc.parallelize(data)\n",
        "new_rdd = rdd.distinct()\n",
        "print(new_rdd.collect())"
      ],
      "metadata": {
        "id": "-4K0Kruirr9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# groupByKey"
      ],
      "metadata": {
        "id": "RQHW3tSDsUVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([('a',1),('a',2),('a',3),('b',1)])\n",
        "res = rdd.groupByKey().mapValues(list)\n",
        "print(res.collect())"
      ],
      "metadata": {
        "id": "JlZc_XTVsGHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## reduceByKey()"
      ],
      "metadata": {
        "id": "agVEZnTXs6hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "\n",
        "rdd = sc.parallelize([('a',1),('a',2),('a',3),('b',1)])\n",
        "res = rdd.reduceByKey(add)\n",
        "print(res.collect())"
      ],
      "metadata": {
        "id": "L6Kwg0P1s1fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sortByKey() == order by"
      ],
      "metadata": {
        "id": "L3rTxl2qtHSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([('a',1),('a',2),('a',3),('b',1)])\n",
        "res = rdd.sortByKey()\n",
        "res.collect()"
      ],
      "metadata": {
        "id": "3kXgYv58tF8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## union()"
      ],
      "metadata": {
        "id": "EQYTbtjxtYd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = sc.parallelize([1, 2, 3, 4,5, 6, 7, 8])\n",
        "rdd2 = sc.parallelize([9, 10, 11, 12,13, 14, 15])\n",
        "new_rdd = rdd1.union(rdd2)\n",
        "new_rdd.collect()"
      ],
      "metadata": {
        "id": "zzadn2K6tZtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actions"
      ],
      "metadata": {
        "id": "7HX7vPIblUld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## count()"
      ],
      "metadata": {
        "id": "f1ED478kl24A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1,1,1,4,5,6,7,8,9,  10]\n",
        "myRdd = sc.parallelize(data)\n",
        "\n",
        "myRdd.count()"
      ],
      "metadata": {
        "id": "MR9m44wutmhu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4e1d82c-439e-4876-f9d2-858e29a308a3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## reduce()"
      ],
      "metadata": {
        "id": "OdhTSouRl5GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1,2,3,4,5]\n",
        "myRdd = sc.parallelize(data)\n",
        "\n",
        "myRdd.reduce(lambda x, y : x * y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaGetqb6lhDM",
        "outputId": "06296c7a-451c-45e0-b50a-d9dd6a782fec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## foreach()"
      ],
      "metadata": {
        "id": "Tv4Te7w5l-Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fun(x):\n",
        "  print(x)\n",
        "\n",
        "data = ['Scala', 'Python', 'Java', 'R']\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "rdd.foreach(fun)"
      ],
      "metadata": {
        "id": "Sbu1s_6AlujX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## countByValue()"
      ],
      "metadata": {
        "id": "z1Ec_YoKmhGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = ['Scala', 'Python', 'Java', 'R', 'Python', 'Java', 'R']\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "rdd.countByValue().items()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ59hAJKmSH-",
        "outputId": "c03ad990-2c39-4dd1-e664-c8d90f40321c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('Scala', 1), ('Python', 2), ('Java', 2), ('R', 2)])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## countbyKey()"
      ],
      "metadata": {
        "id": "Cje8mD9bmvGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('a',1),('b',1),('c',1),('a',1)]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "rdd.countByKey()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKIv39nQmtRA",
        "outputId": "445381df-8002-48d1-87eb-45f4902910e4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {'a': 2, 'b': 1, 'c': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## take(n)"
      ],
      "metadata": {
        "id": "yVZ_x9SunWOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [ 2,5,3,8,4]\n",
        "rdd = sc.parallelize(data)\n",
        "rdd.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLUfEtNbnQUC",
        "outputId": "6d13ce8a-e933-4456-e6c4-de53cd9e4118"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 5, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [ 2,5,3,8,4]\n",
        "rdd = sc.parallelize(data)\n",
        "rdd.top(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvumlxJyngRp",
        "outputId": "585bab59-88e3-4317-a3be-1e533c85b234"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 5, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Tutorial').getOrCreate()\n",
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "rdd = spark.sparkContext.parallelize(dept)"
      ],
      "metadata": {
        "id": "v8spxwSWni_G"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = rdd.toDF()\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJcAbkh6n1Td",
        "outputId": "3d3a5f84-7959-44f2-ce71-b36be4b25780"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n",
            "+---------+---+\n",
            "|_1       |_2 |\n",
            "+---------+---+\n",
            "|Finance  |10 |\n",
            "|Marketing|20 |\n",
            "|Sales    |30 |\n",
            "|IT       |40 |\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "df2 = rdd.toDF(deptColumns)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBOSm35on6Jp",
        "outputId": "6de89713-4a76-4114-e1c6-55e910390292"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8-59yNXoBNE",
        "outputId": "620b315b-7949-4ddb-cae3-82eb3b0f10e6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType\n",
        "deptSchema = StructType([\n",
        "    StructField('dept_name', StringType(), True),\n",
        "    StructField('dept_id', StringType(), True)\n",
        "])\n",
        "\n",
        "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
        "deptDF1.printSchema()\n",
        "deptDF1.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gUDMPL2oEz8",
        "outputId": "39dad899-74f4-4c8b-a62e-168d748dd9cb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: string (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wb21WtFcoKnM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}